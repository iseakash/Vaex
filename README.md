# Vaex
Reading and Processing Millions of Dataset (Big Data) within Seconds

1. Create datset with million of rows.
2. Creating a csv file.
3. Check memory usage.
4. Convert csv to hdf5 for using with vaex as well as with S3 bucket of AWS.
5. Transform data (lazily) whenever needed to save memory.
6. Check time of execution for different operations using time library.
7. Out of core dataframe (filtering and evaluating expressions, not wasting memory by making copies.
8. Algorithm work out of core, limit is size of the harddriver.
9. Fast Groupby/ Aggregations.
10. Try on Newyork Taxi Datset (around 113 GB dataset) from vaex website.
